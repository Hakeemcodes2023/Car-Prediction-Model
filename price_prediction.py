# -*- coding: utf-8 -*-
"""Price Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YqXLLvI3EKO4qkM0saoQvGZu-V52ngsI
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from scipy.stats import skewnorm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from statsmodels.graphics.mosaicplot import mosaic
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/Ai_data_set_third_year_project.csv'

df = pd.read_csv(file_path)
df.head()

# Print Column Names
df.columns

df.value_counts()

df['Manufacturer'].value_counts()

df.dtypes

print("\nSummary Statistics:")
print(df.describe())

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Label Encoding for Specific Categorical Columns
label_encoders = {}
columns_to_encode = ['Manufacturer', 'transmission', 'model', 'fuelType']
existing_columns_to_encode = [col for col in columns_to_encode if col in df.columns]

for col in existing_columns_to_encode:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le

if existing_columns_to_encode:
    print("\nEncoded Columns after Label Encoding:")
    print(df[existing_columns_to_encode].head())
else:
    print("\nNo specified columns found for label encoding.")

# Univariate Analysis
print("\nUnivariate Analysis:")
num_columns = df.select_dtypes(include=['int64', 'float64']).columns

# Histograms
for col in num_columns:
    plt.figure(figsize=(6, 4))
    sns.histplot(df[col], kde=True, bins=30, color='blue')
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

# Box Plots for Outliers
for col in num_columns:
    plt.figure(figsize=(6, 4))
    sns.boxplot(x=df[col], color='cyan')
    plt.title(f'Boxplot of {col}')
    plt.xlabel(col)
    plt.show()

# Multivariate Analysis
print("\nMultivariate Analysis:")

# Correlation Matrix
plt.figure(figsize=(10, 8))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

# Specific Questions Analysis
questions = {
    "Does the year of the car result in a higher price?": "Year",
    "Does the model of the car result in a larger price or a lower price?": "Model",
    "Would the type of transmission in the car result in lower or higher price?": "Transmission",
    "Would a higher mileage lower the price of a car?": "Mileage",
    "Would the type of fuel that the cars need affect the price of the car?": "Fuel_Type",
    "Does cars with a higher price range have a higher road tax?": "Road_Tax",
    "Does a car with a higher mpg tend to have a higher price?": "MPG"
}

if 'price' in df.columns:
    for question, feature in questions.items():
        if feature in df.columns:
            plt.figure(figsize=(8, 6))
            if df[feature].dtype in ['int64', 'float64']:
                sns.scatterplot(x=df[feature], y=df['price'], color='purple')
                plt.title(f'{feature} vs Price')
                plt.xlabel(feature)
                plt.ylabel('Price')
                plt.show()
            else:
                sns.boxplot(x=df[feature], y=df['price'], palette='Set2')
                plt.title(f'{feature} vs Price')
                plt.xlabel(feature)
                plt.ylabel('Price')
                plt.show()

# Analysis of Model vs Price
if 'Model' in df.columns and 'price' in df.columns:
    plt.figure(figsize=(12, 6))
    sns.boxplot(x=df['Model'], y=df['price'], palette='muted')
    plt.title('Model vs Price')
    plt.xlabel('Model')
    plt.ylabel('Price')
    plt.xticks(rotation=90)
    plt.show()

# Analysis of Mileage vs Price
if 'Mileage' in df.columns and 'price' in df.columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=df['Mileage'], y=df['price'], palette='coolwarm')
    plt.title('Mileage vs Price')
    plt.xlabel('Mileage')
    plt.ylabel('Price')
    plt.show()

# Pairplot for Numerical Features
sns.pairplot(df[num_columns], diag_kind='kde')
plt.show()

numerical_features = ['year', 'tax', 'mpg', 'engineSize']

if 'price' in df.columns:
    plt.figure(figsize=(8, 5))
    sns.histplot(df['price'], kde=True, bins=30)
    plt.title("Price Distribution")
    plt.xlabel("Price")
    plt.ylabel("Frequency")
    plt.show()

#Scatter Plot
for feature in numerical_features:
    plt.figure(figsize=(8, 6))
    sns.scatterplot(data=df, x=feature, y='price')
    plt.title(f"Scatter Plot: Price vs {feature}")
    plt.xlabel(feature)
    plt.ylabel("Price")
    plt.grid(True)
    plt.show()

#Box Plot
categorical_features = ['transmission', 'fuelType', 'Manufacturer']

for feature in categorical_features:
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=df, x=feature, y='price')
    plt.title(f"Box Plot: Price by {feature.capitalize()}")
    plt.xlabel(feature.capitalize())
    plt.ylabel("Price")
    plt.xticks(rotation=45)
    plt.show()

numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
if len(numerical_columns) > 1:
    sns.pairplot(df[numerical_columns], diag_kind='kde')
    plt.show()

#Relationship with price
if 'price' in df.columns:
    for column in numerical_columns:
        if column != 'price':
            plt.figure(figsize=(8, 5))
            sns.scatterplot(data=df, x=column, y='price')
            plt.title(f"Price vs {column}")
            plt.xlabel(column)
            plt.ylabel("Price")
            plt.show()

categorical_columns = df.select_dtypes(include=['object']).columns
for column in categorical_columns:
    if column != 'price':
        plt.figure(figsize=(8, 5))
        sns.boxplot(data=df, x=column, y='price')
        plt.title(f"Price Distribution by {column}")
        plt.xlabel(column)
        plt.ylabel("Price")
        plt.xticks(rotation=45)
        plt.show()

# Drop the 'Tax' feature from the dataset
if 'tax' in df.columns:
    df.drop(columns=['tax'], inplace=True)
    print("The 'Tax' feature has been dropped.")
else:
    print("'Tax' column does not exist in the dataset.")

df.head(5)

# Identify and Remove Outliers
def remove_outliers_iqr(df, numerical_columns):
    clean_df = df.copy()
    for col in numerical_columns:
        Q1 = clean_df[col].quantile(0.25)
        Q3 = clean_df[col].quantile(0.75)
        IQR = Q3 - Q1  # Interquartile Range

        # Define the outlier range
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Remove rows with outliers
        clean_df = clean_df[(clean_df[col] >= lower_bound) & (clean_df[col] <= upper_bound)]
        print(f"Outliers removed from '{col}': {df.shape[0] - clean_df.shape[0]} rows")

    return clean_df

# Select numerical columns
numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns

# Remove outliers
df_cleaned = remove_outliers_iqr(df, numerical_columns)

print(f"Dataset shape before removing outliers: {df.shape}")
print(f"Dataset shape after removing outliers: {df_cleaned.shape}")

df = df.drop_duplicates()
print(df.isnull().sum())

df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)


numeric_columns = ['year', 'price', 'mpg', 'engineSize']
for col in numeric_columns:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')


df = df.dropna()
df

if 'year' in df.columns:
    df = df[(df['year'] >= 2010) & (df['year'] <= 2020)]
if 'engine_size' in df.columns:
    df = df[df['engine_size'] > 0]

df.columns = df.columns.str.strip()
df

from sklearn.model_selection import train_test_split

# Define the target variable
target_column = 'price'
X = df.drop(columns=[target_column])  # Features
y = df[target_column]  # Target variable


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Further split the training set into training and validation sets (e.g., 70% train, 10% validation)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=42)


print(f"Training set size: {X_train.shape[0]} rows")
print(f"Validation set size: {X_val.shape[0]} rows")
print(f"Testing set size: {X_test.shape[0]} rows")

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)
X_standardized = pd.DataFrame(X_standardized, columns=X.columns)

# Display the standardized data
print("Standardized Data (First 5 Rows):")
print(X_standardized.head())

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression

# Random Forest Regressor
random_forest_model = RandomForestRegressor(random_state=42, n_estimators=100)
random_forest_model.fit(X_train, y_train)
rf_val_predictions = random_forest_model.predict(X_val)

# Linear Regression
linear_regression_model = LinearRegression()
linear_regression_model.fit(X_train, y_train)
lr_val_predictions = linear_regression_model.predict(X_val)

# Combine predictions from Random Forest and Linear Regression as meta-features for Validation Set
meta_features_val = np.column_stack((rf_val_predictions, lr_val_predictions))

# Gradient Boosting Regressor (Meta-Model)
meta_model = GradientBoostingRegressor(random_state=42, n_estimators=100)
meta_model.fit(meta_features_val, y_val)

# Testing Phase: Generate predictions on the test set
rf_test_predictions = random_forest_model.predict(X_test)
lr_test_predictions = linear_regression_model.predict(X_test)

# Combine test predictions for meta-model
meta_test_features = np.column_stack((rf_test_predictions, lr_test_predictions))
meta_predictions = meta_model.predict(meta_test_features)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Evaluation Metrics
mae = mean_absolute_error(y_test, meta_predictions)
mse = mean_squared_error(y_test, meta_predictions)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, meta_predictions)

print("Evaluation Metrics:")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R^2): {r2}")

# Visualize predictions vs actual values
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.scatter(y_test, meta_predictions, alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', lw=2, linestyle='--')
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Meta-Model Predictions vs Actual Values")
plt.show()

# Predictions on Seen Data (Training Set)
rf_train_predictions = random_forest_model.predict(X_train)
lr_train_predictions = linear_regression_model.predict(X_train)


meta_train_features = np.column_stack((rf_train_predictions, lr_train_predictions))


meta_train_predictions = meta_model.predict(meta_train_features)

# Evaluation Metrics on Training Data
mae_train = mean_absolute_error(y_train, meta_train_predictions)
mse_train = mean_squared_error(y_train, meta_train_predictions)
rmse_train = np.sqrt(mse_train)
r2_train = r2_score(y_train, meta_train_predictions)

print("Evaluation Metrics on Seen Data (Training Set):")
print(f"Mean Absolute Error (MAE): {mae_train}")
print(f"Mean Squared Error (MSE): {mse_train}")
print(f"Root Mean Squared Error (RMSE): {rmse_train}")
print(f"R-squared (R^2): {r2_train}")

plt.figure(figsize=(10, 6))
plt.scatter(y_train, meta_train_predictions, alpha=0.7, color='blue')
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], color='red', lw=2, linestyle='--')
plt.xlabel("Actual Values (Training Set)")
plt.ylabel("Predicted Values (Training Set)")
plt.title("Meta-Model Predictions vs Actual Values on Training Data")
plt.show()

y_val_predictions = random_forest_model.predict(X_val)

# Step 3: Calculate residuals
residuals = y_val - y_val_predictions

# Step 4: Plot residuals
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_val_predictions, y=residuals, alpha=0.6)
plt.axhline(y=0, color='red', linestyle='--')  # Horizontal line at 0 for reference
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Residual Plot: Predicted Values vs Residuals")
plt.show()

import seaborn as sns
sns.histplot(residuals , kde=True, bins=30, color='blue')
plt.title("Residual Distribution")
plt.xlabel("Residuals")
plt.ylabel("Frequency")
plt.show()